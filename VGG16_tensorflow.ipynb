{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Train IELM and evaluating training accuracy\n",
        "no_classes = 8\n",
        "error = 0.1\n",
        "loss_function = \"binary_cross_entropy\" #\"mean_squared_error\"  #It can be mean_absolute_error also\n",
        "activation_function = \"tanh\"\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loading_data(train_path,test_path):\n",
        "\n",
        "    # Define the data transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images to (224, 224)\n",
        "        transforms.ToTensor(),  # Convert images to tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
        "    ])\n",
        "\n",
        "    \n",
        "\n",
        "    train_dataset = ImageFolder(train_path, transform=transform)\n",
        "    traindata_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_dataset = ImageFolder(test_path, transform=transform)\n",
        "    testdata_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "    return traindata_loader,testdata_loader\n",
        "\n",
        "def load_model():\n",
        "    #loading of model\n",
        "    device=torch.device(\"cuda\")  \n",
        "    import torchvision.models as models\n",
        "    vgg_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    vgg_model = torch.nn.Sequential(*list(vgg_model.features.children()))\n",
        "    vgg_model=vgg_model.to(device)\n",
        "    vgg_model.eval()\n",
        "    return vgg_model\n",
        "#training features extraction\n",
        "def training_feature_extraction(device, traindata_loader,vgg_model):\n",
        "\n",
        "    x_training_features = []\n",
        "    y_train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in traindata_loader:        \n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Extract features using VGG16\n",
        "            with torch.no_grad():\n",
        "                features = vgg_model(images)   \n",
        "            \n",
        "            x_training_features.append(features)\n",
        "            y_train_labels.append(labels)\n",
        "    x_training_features = torch.cat(x_training_features, dim=0)\n",
        "    y_train_labels = torch.cat(y_train_labels, dim=0)\n",
        "\n",
        "    x_training_features.to(device)\n",
        "    y_train_labels.to(device)\n",
        "    return x_training_features,y_train_labels\n",
        "\n",
        "def testing_feature_extratcion(device,testdata_loader,vgg_model):\n",
        "    x_testing_features = []\n",
        "    y_testing_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testdata_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Extract features using VGG16\n",
        "            with torch.no_grad():\n",
        "                features = vgg_model(images) \n",
        "            x_testing_features.append(vgg_model(images))\n",
        "            y_testing_labels.append(labels)\n",
        "    x_testing_features = torch.cat(x_testing_features, dim=0)\n",
        "    y_testing_labels = torch.cat(y_testing_labels, dim=0)\n",
        "\n",
        "    x_testing_features.to(device)\n",
        "    y_testing_labels.to(device)\n",
        "    return x_testing_features,y_testing_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "IELM Constructor class\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "class I_ELM():\n",
        "    \"\"\" Constructor to initialize node\"\"\"\n",
        "    def __init__(self, no_input_nodes, max_no_hidden_nodes, no_output_nodes,\n",
        "        activation_function='sigmoid', loss_function='mean_squared_error'):\n",
        "        device=torch.device(\"cuda\") \n",
        "        \n",
        "        self.no_input_nodes = no_input_nodes\n",
        "        self.no_hidden_nodes = 1\n",
        "        self.no_output_nodes = no_output_nodes\n",
        "\n",
        "        self.beta = torch.FloatTensor(self.no_hidden_nodes, self.no_output_nodes).uniform_(-1., 1.)\n",
        "        self.beta=self.beta.to(device)\n",
        "        \n",
        "        # initialize weights between Input Layer and hidden layer\n",
        "        self.alpha=torch.FloatTensor(self.no_input_nodes, self.no_hidden_nodes).uniform_(-1.,1.)\n",
        "        self.alpha=self.alpha.to(device)\n",
        "        \n",
        "        # Initialize Biases\n",
        "        self.bias =torch.zeros(size=(self.no_hidden_nodes,))\n",
        "        self.bias=self.bias.to(device)\n",
        "        # set an activation function\n",
        "        self.activation_function = activation_function\n",
        "       \n",
        "        # set a loss function\n",
        "        self.loss_function = loss_function\n",
        "    \n",
        "    def mean_squared_error(self,Y_True, Y_Pred):\n",
        "        return 0.5 * torch.mean((Y_True - Y_Pred)**2)\n",
        "\n",
        "    def mean_absolute_error(self, Y_True, Y_Pred):\n",
        "        return torch.mean(torch.abs(Y_True - Y_Pred))\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1. /(1.+ torch.exp(-x))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return torch.tensor(self(X), dtype=torch.float32)\n",
        "    \n",
        "    def __call__(self, X):\n",
        "        h = torch.sigmoid(torch.matmul(X,self.alpha) + self.bias)\n",
        "        return torch.matmul(h,self.beta)\n",
        "    \n",
        "    def binary_cross_entropy(self,Y_true, Y_pred):\n",
        "        lss=[]\n",
        "        Y_pred = torch.clamp(Y_pred, min=1e-7, max=1.0 - 1e-7)\n",
        "        loss_calculated=- (Y_true * torch.log(Y_pred) + (1 - Y_true) * torch.log(1 - Y_pred))\n",
        "        '''loss_calculated_max=torch.argmax(loss_calculated, axis=-1)\n",
        "        loss_calculated_max=loss_calculated_max.cpu()\n",
        "        \n",
        "        for i in range(len(loss_calculated_max)):\n",
        "            l1=loss_calculated_max[i]\n",
        "            lss.append(l1)\n",
        "        #print(lss)\n",
        "        plt.plot(range(1, Lmax), lss)\n",
        "        plt.xlabel('Hidden Layer Neurons')\n",
        "        plt.ylabel('Training Loss')\n",
        "        #plt.title('BCE Loss Over Epochs')\n",
        "        plt.show()\n",
        "        '''\n",
        "        loss=torch.mean(loss_calculated)\n",
        "        return loss\n",
        "\n",
        "    def plot_confusion(self,Y_true, Y_pred):\n",
        "         \n",
        "        # Calculate the confusion matrix\n",
        "        #y_pred_score= \n",
        "        confusion = confusion_matrix(Y_true, Y_pred)\n",
        "        \n",
        "        # Extract values from the confusion matrix\n",
        "        TP = confusion[1, 1]\n",
        "        FP = confusion[0, 1]\n",
        "        TN = confusion[0, 0]\n",
        "        FN = confusion[1, 0]\n",
        "        TPR= TP/(TP+FN)\n",
        "        FPR=FP/(FP+TN)\n",
        "        X=[0,FPR,1]\n",
        "        Y=[0,TPR,1]\n",
        "        AUCSCORE =np.trapz(Y,X)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score\n",
        "        precision, recall, f1_score, _ = precision_recall_fscore_support(Y_true,Y_pred, average='macro')\n",
        "\n",
        "        print(\"precision=\",precision,\"recall=\",recall,\"f1-score=\",f1_score,\"auc-score=\",AUCSCORE)\n",
        "\n",
        "        Sensitivity =TP/(TP+FN)\n",
        "        Specificity =TN/(TN+FP)\n",
        "        fpr = 1-Specificity\n",
        "        fpr = 1-Specificity\n",
        "        print(\"sensitivity=\",Sensitivity,\"specificity=\",Specificity,\"fpr=\",fpr)\n",
        "        # Print the confusion matrix\n",
        "        print(\"Confusion Matrix:\",confusion)\n",
        "         \n",
        "        import pandas as pd\n",
        "\n",
        "        data = {'Column1':[precision,recall,f1_score,AUCSCORE,Sensitivity,Specificity,fpr]}\n",
        "        df_matric = pd.DataFrame(data=data)\n",
        "        df_matric.index  = ['Precision', 'recall', 'f1-score','Accuracy Score','sensitivity','specificity','fpr']\n",
        "        print(df_matric)\n",
        "\n",
        "        \n",
        "        # Get the number of classes\n",
        "        num_classes = len(np.unique(Y_true))\n",
        "\n",
        "        # Create a figure and axis\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots(figsize=(4,4))\n",
        "        cmlabels=['Apple','Corn','Grape','Potato','Rice','Tea','Tomato','Wheat']\n",
        "        # Plot the confusion matrix using a heatmap\n",
        "        sns.heatmap(confusion, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, ax=ax,xticklabels=cmlabels,yticklabels=cmlabels)\n",
        "\n",
        "        # Set axis labels and title\n",
        "        ax.set_xlabel(\"Predicted labels\")\n",
        "        ax.set_ylabel(\"Actual labels\")\n",
        "        title=\"CM for Model 02 VGG16\"\n",
        "        ax.set_title(\"\")\n",
        "        plt.savefig(title+'confusion_matrix.png')\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, X, Y_true, metrics):\n",
        "        Y_pred =self.predict(X)\n",
        "        #Y_true = Y_true\n",
        "        \n",
        "        ret = []\n",
        "        \n",
        "        Y_pred=torch.tensor(Y_pred,dtype=torch.float32)\n",
        "\n",
        "\n",
        "        for m in metrics:\n",
        "            if m == 'loss':\n",
        "                loss=[]\n",
        "                loss = self.binary_cross_entropy(Y_true, Y_pred)\n",
        "                ret.append(loss)                \n",
        "            elif m == 'accuracy':\n",
        "                Y_pred_argmax = torch.argmax(Y_pred, axis=-1)\n",
        "                Y_true_argmax = torch.argmax(Y_true, axis=-1)\n",
        "                acc = torch.sum(Y_pred_argmax == Y_true_argmax) / len(Y_true)\n",
        "                #plotting of confusion matrix\n",
        "\n",
        "                # Get predicted probabilities for positive class\n",
        "                Y_true_argmax=Y_true_argmax.cpu()\n",
        "                Y_pred_argmax=Y_pred_argmax.cpu()\n",
        "                #self.plot_confusion(Y_true_argmax, Y_pred_argmax)\n",
        "                ret.append(acc)\n",
        "            else:\n",
        "                raise ValueError('an unknown evaluation indicator \\'%s\\'.' % m)\n",
        "        if len(ret) == 1:\n",
        "            print(ret)\n",
        "            ret = ret\n",
        "        elif len(ret) == 0:\n",
        "            ret = None\n",
        "        return ret\n",
        "\n",
        "    def fit(self, X, Y_true,Lmax,error,activation):\n",
        "        device=torch.device(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            self.beta=torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "            self.alpha = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "            #print(self.beta.shape, self.alpha.shape,X.shape)\n",
        "            \n",
        "            self.alpha=self.alpha.to(device)\n",
        "            self.beta=self.beta.to(device)\n",
        "            if(activation=='s'):\n",
        "                H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "            elif(activation=='t'):\n",
        "                H = torch.tanh(torch.matmul(X, self.alpha))  \n",
        "            elif(activation=='r'):\n",
        "                H = torch.relu(torch.matmul(X, self.alpha))\n",
        "            \n",
        "            \n",
        "            # Compute a pseudoinverse of H\n",
        "            H_pinv = torch.pinverse(H)\n",
        "\n",
        "            # Update beta\n",
        "            self.beta = torch.matmul(H_pinv, Y_true.float())\n",
        "            \n",
        "            for i in range(2, Lmax):\n",
        "                beta_random = torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "                alpha_random = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "                \n",
        "                beta_random=beta_random.to(device)\n",
        "                alpha_random=alpha_random.to(device)\n",
        "                \n",
        "                self.alpha=torch.cat((self.alpha,alpha_random),dim=1)                          \n",
        "                self.beta =torch.cat((self.beta,beta_random),dim=0)\n",
        "\n",
        "                if(activation=='s'):\n",
        "                    H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "                elif(activation=='t'):\n",
        "                    H = torch.tanh(torch.matmul(X, self.alpha))  \n",
        "                elif(activation=='r'):\n",
        "                    H = torch.relu(torch.matmul(X, self.alpha))\n",
        "                \n",
        "                H_pinv = torch.pinverse(H)\n",
        "                self.beta = torch.matmul(H_pinv, Y_true)\n",
        "\n",
        "from  keras.utils import np_utils\n",
        "def train_features_conversion(x_training_features,y_train_labels):\n",
        "\n",
        "    # Convert the extracted features to numpy arrays\n",
        "\n",
        "    device=torch.device(\"cuda\")\n",
        "    x_training_features_np = x_training_features.detach().cpu().numpy()  \n",
        "    flattened_features_train = x_training_features_np.reshape(x_training_features_np.shape[0], -1)\n",
        "    train_features_tensor = torch.tensor(flattened_features_train, dtype=torch.float32).to(device)\n",
        "    \n",
        "    y_train_array= y_train_labels.cpu().detach().numpy()\n",
        "    y_train_array_encoded = np_utils.to_categorical(y_train_array,no_classes)\n",
        "    y_train_labels_encoded=torch.tensor(y_train_array_encoded,dtype=torch.float32)\n",
        "    y_train_labels_encoded=y_train_labels_encoded.to(device)    \n",
        "    return train_features_tensor,y_train_labels_encoded\n",
        "\n",
        "def test_features_conversion(x_testing_features,y_testing_labels):\n",
        "    # Convert the extracted features to numpy arrays\n",
        "    device=torch.device(\"cuda\")\n",
        "    x_testing_features_np = x_testing_features.detach().cpu().numpy()\n",
        "    \n",
        "    flattened_features_test = x_testing_features_np.reshape(x_testing_features_np.shape[0], -1)\n",
        "    test_features_tensor = torch.tensor(flattened_features_test,dtype=torch.float32).to(device)\n",
        "    \n",
        "    y_test_array= y_testing_labels.cpu().detach().numpy()\n",
        "    y_test_array_encoded = np_utils.to_categorical(y_test_array,no_classes)\n",
        "    y_test_labels_encoded=torch.tensor(y_test_array_encoded,dtype=torch.float32)\n",
        "    y_test_labels_encoded=y_test_labels_encoded.to(device)\n",
        "    return test_features_tensor, y_test_labels_encoded\n",
        "\n",
        "def fit_neuron(self, X, Y_true,Lmax,error,activation):\n",
        "        device=torch.device(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            self.beta=torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "            self.alpha = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "            #print(self.beta.shape, self.alpha.shape,X.shape)\n",
        "            \n",
        "            self.alpha=self.alpha.to(device)\n",
        "            self.beta=self.beta.to(device)\n",
        "            if(activation=='s'):\n",
        "                H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "            elif(activation=='t'):\n",
        "                H = torch.tanh(torch.matmul(X, self.alpha))  \n",
        "            elif(activation=='r'):\n",
        "                H = torch.relu(torch.matmul(X, self.alpha))\n",
        "            \n",
        "            \n",
        "            # Compute a pseudoinverse of H\n",
        "            H_pinv = torch.pinverse(H)\n",
        "\n",
        "            # Update beta\n",
        "            self.beta = torch.matmul(H_pinv, Y_true.float())\n",
        "            \n",
        "            for i in range(2, Lmax):\n",
        "                beta_random = torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "                alpha_random = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "                \n",
        "                beta_random=beta_random.to(device)\n",
        "                alpha_random=alpha_random.to(device)\n",
        "                \n",
        "                self.alpha=torch.cat((self.alpha,alpha_random),dim=1)                          \n",
        "                self.beta =torch.cat((self.beta,beta_random),dim=0)\n",
        "\n",
        "                if(activation=='s'):\n",
        "                    H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "                elif(activation=='t'):\n",
        "                    H = torch.tanh(torch.matmul(X, self.alpha))  \n",
        "                elif(activation=='r'):\n",
        "                    H = torch.relu(torch.matmul(X, self.alpha))\n",
        "                \n",
        "                H_pinv = torch.pinverse(H)\n",
        "                self.beta = torch.matmul(H_pinv, Y_true)\n",
        "                \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\") \n",
        "train_path='D:/Smart_Farming_Capstone_Project_G201/Model02/Dataset/Type_of_plant/Train/'\n",
        "test_path='D:/Smart_Farming_Capstone_Project_G201/Model02/Dataset/Type_of_plant/Test/'\n",
        "traindata_loader,testdata_loader=loading_data(train_path,test_path)\n",
        "model=load_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_training_features,y_train_labels=training_feature_extraction(device,traindata_loader,model)\n",
        "train_features_tensor,y_train_labels_encoded=train_features_conversion(x_training_features,y_train_labels)\n",
        "torch.save(train_features_tensor,'D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Train/train_featuresVGG16.pth')\n",
        "torch.save(y_train_labels_encoded,'D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Train/train_labelsVGG16.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_testing_features,y_testing_labels=testing_feature_extratcion(device,testdata_loader,model)\n",
        "test_features_tensor, y_test_labels_encoded=test_features_conversion(x_testing_features,y_testing_labels)\n",
        "torch.save(test_features_tensor,\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Test/testing_featuresVGG16.pth\")\n",
        "torch.save(y_test_labels_encoded,\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Test/testing_labelsVGG16.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def i_elmTraining(features_train,y_train,activation):\n",
        "  # ===============================\n",
        "  # Instantiate ELM object\n",
        "  # ===============================\n",
        "  input=features_train.size(1)\n",
        "  ielm_model = I_ELM(\n",
        "      no_input_nodes=input,\n",
        "      max_no_hidden_nodes=Lmax,\n",
        "      no_output_nodes=no_classes,\n",
        "      loss_function=loss_function,\n",
        "      activation_function=activation_function,    \n",
        "  )\n",
        "\n",
        "  i = time.time() \n",
        "  ielm_model.fit(features_train,y_train,Lmax,error,activation)\n",
        "  final = time.time()\n",
        "\n",
        "  training_loss, training_acc = ielm_model.evaluate(features_train, y_train, metrics=['loss','accuracy'])\n",
        "  \n",
        "  print('Training Loss in mean square error: %f' % training_loss) # loss value\n",
        "  print('Training Accuracy: %f' % training_acc)# accuracy\n",
        "  print('Total Time require for Training %f Seconds'% (final-i))\n",
        "\n",
        "  return ielm_model\n",
        "def i_elmTesting(ielm_model,features_test,y_test):\n",
        "  \n",
        "  i = time.time()\n",
        "  test_loss, test_acc = ielm_model.evaluate(features_test, y_test, metrics=['loss', 'accuracy'])\n",
        "  final = time.time()\n",
        "  \n",
        "  print('Testing Loss in mean square error: %f ' % test_loss,\"\\n\")\n",
        "  print('Testing Accuracy: %f' % test_acc)\n",
        "  time_taken=(final-i)/features_test.shape[0]\n",
        "  print('The testing time is ',(final-i))\n",
        "  print('Total Time require for Testing one image is ' ,time_taken ,'seconds')\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_training_features=torch.load('D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Train/train_featuresVGG16.pth')\n",
        "y_train_labels=torch.load('D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Train/train_labelsVGG16.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x_testing_features=torch.load('D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Test/testing_featuresVGG16.pth')\n",
        "y_testing_labels=torch.load('D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/VGG16/Test/testing_labelsVGG16.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dr. Rajendra Ku Roul\\AppData\\Local\\Temp\\ipykernel_22308\\3349744081.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(self(X), dtype=torch.float32)\n",
            "C:\\Users\\Dr. Rajendra Ku Roul\\AppData\\Local\\Temp\\ipykernel_22308\\3349744081.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_pred=torch.tensor(Y_pred,dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no of neuron= 0 Testing Accuracy: 0.129333\n",
            "no of neuron= 1 Testing Accuracy: 0.119778\n",
            "no of neuron= 2 Testing Accuracy: 0.128444\n",
            "no of neuron= 3 Testing Accuracy: 0.161333\n",
            "no of neuron= 4 Testing Accuracy: 0.216222\n",
            "no of neuron= 5 Testing Accuracy: 0.224778\n",
            "no of neuron= 6 Testing Accuracy: 0.295778\n",
            "no of neuron= 7 Testing Accuracy: 0.250444\n",
            "no of neuron= 8 Testing Accuracy: 0.304556\n",
            "no of neuron= 9 Testing Accuracy: 0.274000\n",
            "no of neuron= 10 Testing Accuracy: 0.312667\n",
            "no of neuron= 11 Testing Accuracy: 0.303000\n",
            "no of neuron= 12 Testing Accuracy: 0.337556\n",
            "no of neuron= 13 Testing Accuracy: 0.403556\n",
            "no of neuron= 14 Testing Accuracy: 0.371000\n",
            "no of neuron= 15 Testing Accuracy: 0.402889\n",
            "no of neuron= 16 Testing Accuracy: 0.374111\n",
            "no of neuron= 17 Testing Accuracy: 0.368778\n",
            "no of neuron= 18 Testing Accuracy: 0.436556\n",
            "no of neuron= 19 Testing Accuracy: 0.433333\n",
            "no of neuron= 20 Testing Accuracy: 0.454111\n",
            "no of neuron= 21 Testing Accuracy: 0.382000\n",
            "no of neuron= 22 Testing Accuracy: 0.421444\n",
            "no of neuron= 23 Testing Accuracy: 0.483778\n",
            "no of neuron= 24 Testing Accuracy: 0.490444\n",
            "no of neuron= 25 Testing Accuracy: 0.449778\n",
            "no of neuron= 26 Testing Accuracy: 0.458556\n",
            "no of neuron= 27 Testing Accuracy: 0.497222\n",
            "no of neuron= 28 Testing Accuracy: 0.505000\n",
            "no of neuron= 29 Testing Accuracy: 0.514778\n",
            "no of neuron= 30 Testing Accuracy: 0.526222\n",
            "no of neuron= 31 Testing Accuracy: 0.523778\n",
            "no of neuron= 32 Testing Accuracy: 0.520444\n",
            "no of neuron= 33 Testing Accuracy: 0.544667\n",
            "no of neuron= 34 Testing Accuracy: 0.520000\n",
            "no of neuron= 35 Testing Accuracy: 0.540778\n",
            "no of neuron= 36 Testing Accuracy: 0.583222\n",
            "no of neuron= 37 Testing Accuracy: 0.535889\n",
            "no of neuron= 38 Testing Accuracy: 0.573556\n",
            "no of neuron= 39 Testing Accuracy: 0.565111\n",
            "no of neuron= 40 Testing Accuracy: 0.537111\n",
            "no of neuron= 41 Testing Accuracy: 0.581000\n",
            "no of neuron= 42 Testing Accuracy: 0.586444\n",
            "no of neuron= 43 Testing Accuracy: 0.571778\n",
            "no of neuron= 44 Testing Accuracy: 0.596667\n",
            "no of neuron= 45 Testing Accuracy: 0.571444\n",
            "no of neuron= 46 Testing Accuracy: 0.630000\n",
            "no of neuron= 47 Testing Accuracy: 0.586000\n",
            "no of neuron= 48 Testing Accuracy: 0.605556\n",
            "no of neuron= 49 Testing Accuracy: 0.637889\n",
            "no of neuron= 50 Testing Accuracy: 0.606778\n",
            "no of neuron= 51 Testing Accuracy: 0.616444\n",
            "no of neuron= 52 Testing Accuracy: 0.611444\n",
            "no of neuron= 53 Testing Accuracy: 0.634333\n",
            "no of neuron= 54 Testing Accuracy: 0.612333\n",
            "no of neuron= 55 Testing Accuracy: 0.643111\n",
            "no of neuron= 56 Testing Accuracy: 0.647333\n",
            "no of neuron= 57 Testing Accuracy: 0.653444\n",
            "no of neuron= 58 Testing Accuracy: 0.635889\n",
            "no of neuron= 59 Testing Accuracy: 0.657556\n",
            "no of neuron= 60 Testing Accuracy: 0.635444\n",
            "no of neuron= 61 Testing Accuracy: 0.659111\n",
            "no of neuron= 62 Testing Accuracy: 0.646778\n",
            "no of neuron= 63 Testing Accuracy: 0.663889\n",
            "no of neuron= 64 Testing Accuracy: 0.645111\n",
            "no of neuron= 65 Testing Accuracy: 0.696111\n",
            "no of neuron= 66 Testing Accuracy: 0.680000\n",
            "no of neuron= 67 Testing Accuracy: 0.679778\n",
            "no of neuron= 68 Testing Accuracy: 0.661667\n",
            "no of neuron= 69 Testing Accuracy: 0.683111\n",
            "no of neuron= 70 Testing Accuracy: 0.673556\n",
            "no of neuron= 71 Testing Accuracy: 0.679667\n",
            "no of neuron= 72 Testing Accuracy: 0.675778\n",
            "no of neuron= 73 Testing Accuracy: 0.681222\n",
            "no of neuron= 74 Testing Accuracy: 0.676889\n",
            "no of neuron= 75 Testing Accuracy: 0.701000\n",
            "no of neuron= 76 Testing Accuracy: 0.724667\n",
            "no of neuron= 77 Testing Accuracy: 0.708889\n",
            "no of neuron= 78 Testing Accuracy: 0.710333\n",
            "no of neuron= 79 Testing Accuracy: 0.696222\n",
            "no of neuron= 80 Testing Accuracy: 0.689778\n",
            "no of neuron= 81 Testing Accuracy: 0.713333\n",
            "no of neuron= 82 Testing Accuracy: 0.724667\n",
            "no of neuron= 83 Testing Accuracy: 0.717333\n",
            "no of neuron= 84 Testing Accuracy: 0.727444\n",
            "no of neuron= 85 Testing Accuracy: 0.717778\n",
            "no of neuron= 86 Testing Accuracy: 0.723444\n",
            "no of neuron= 87 Testing Accuracy: 0.737111\n",
            "no of neuron= 88 Testing Accuracy: 0.728333\n",
            "no of neuron= 89 Testing Accuracy: 0.736778\n",
            "no of neuron= 90 Testing Accuracy: 0.751222\n",
            "no of neuron= 91 Testing Accuracy: 0.727556\n",
            "no of neuron= 92 Testing Accuracy: 0.735889\n",
            "no of neuron= 93 Testing Accuracy: 0.735667\n",
            "no of neuron= 94 Testing Accuracy: 0.734778\n",
            "no of neuron= 95 Testing Accuracy: 0.765222\n",
            "no of neuron= 96 Testing Accuracy: 0.752444\n",
            "no of neuron= 97 Testing Accuracy: 0.741556\n",
            "no of neuron= 98 Testing Accuracy: 0.750444\n",
            "no of neuron= 99 Testing Accuracy: 0.758889\n",
            "no of neuron= 100 Testing Accuracy: 0.762444\n",
            "no of neuron= 101 Testing Accuracy: 0.767222\n",
            "no of neuron= 102 Testing Accuracy: 0.767778\n",
            "no of neuron= 103 Testing Accuracy: 0.761889\n",
            "no of neuron= 104 Testing Accuracy: 0.752333\n",
            "no of neuron= 105 Testing Accuracy: 0.756222\n",
            "no of neuron= 106 Testing Accuracy: 0.760000\n",
            "no of neuron= 107 Testing Accuracy: 0.754778\n",
            "no of neuron= 108 Testing Accuracy: 0.762556\n",
            "no of neuron= 109 Testing Accuracy: 0.757667\n",
            "no of neuron= 110 Testing Accuracy: 0.745889\n",
            "no of neuron= 111 Testing Accuracy: 0.773556\n",
            "no of neuron= 112 Testing Accuracy: 0.780000\n",
            "no of neuron= 113 Testing Accuracy: 0.773889\n",
            "no of neuron= 114 Testing Accuracy: 0.767889\n",
            "no of neuron= 115 Testing Accuracy: 0.779000\n",
            "no of neuron= 116 Testing Accuracy: 0.775889\n",
            "no of neuron= 117 Testing Accuracy: 0.773333\n",
            "no of neuron= 118 Testing Accuracy: 0.786333\n",
            "no of neuron= 119 Testing Accuracy: 0.795778\n",
            "no of neuron= 120 Testing Accuracy: 0.762889\n",
            "no of neuron= 121 Testing Accuracy: 0.777556\n",
            "no of neuron= 122 Testing Accuracy: 0.805222\n",
            "no of neuron= 123 Testing Accuracy: 0.797556\n",
            "no of neuron= 124 Testing Accuracy: 0.784889\n",
            "no of neuron= 125 Testing Accuracy: 0.796444\n",
            "no of neuron= 126 Testing Accuracy: 0.813889\n",
            "no of neuron= 127 Testing Accuracy: 0.786222\n",
            "no of neuron= 128 Testing Accuracy: 0.790556\n",
            "no of neuron= 129 Testing Accuracy: 0.804667\n",
            "no of neuron= 130 Testing Accuracy: 0.807000\n",
            "no of neuron= 131 Testing Accuracy: 0.791444\n",
            "no of neuron= 132 Testing Accuracy: 0.799000\n",
            "no of neuron= 133 Testing Accuracy: 0.805556\n",
            "no of neuron= 134 Testing Accuracy: 0.800667\n",
            "no of neuron= 135 Testing Accuracy: 0.800667\n",
            "no of neuron= 136 Testing Accuracy: 0.803667\n",
            "no of neuron= 137 Testing Accuracy: 0.805222\n",
            "no of neuron= 138 Testing Accuracy: 0.791778\n",
            "no of neuron= 139 Testing Accuracy: 0.786000\n",
            "no of neuron= 140 Testing Accuracy: 0.808667\n",
            "no of neuron= 141 Testing Accuracy: 0.812778\n",
            "no of neuron= 142 Testing Accuracy: 0.803000\n",
            "no of neuron= 143 Testing Accuracy: 0.821889\n",
            "no of neuron= 144 Testing Accuracy: 0.806778\n",
            "no of neuron= 145 Testing Accuracy: 0.808111\n",
            "no of neuron= 146 Testing Accuracy: 0.819222\n",
            "no of neuron= 147 Testing Accuracy: 0.807000\n",
            "no of neuron= 148 Testing Accuracy: 0.802444\n",
            "no of neuron= 149 Testing Accuracy: 0.822778\n",
            "no of neuron= 150 Testing Accuracy: 0.823778\n",
            "no of neuron= 151 Testing Accuracy: 0.825111\n",
            "no of neuron= 152 Testing Accuracy: 0.833222\n",
            "no of neuron= 153 Testing Accuracy: 0.810222\n",
            "no of neuron= 154 Testing Accuracy: 0.816000\n",
            "no of neuron= 155 Testing Accuracy: 0.816111\n",
            "no of neuron= 156 Testing Accuracy: 0.834000\n",
            "no of neuron= 157 Testing Accuracy: 0.832556\n",
            "no of neuron= 158 Testing Accuracy: 0.818333\n",
            "no of neuron= 159 Testing Accuracy: 0.827333\n",
            "no of neuron= 160 Testing Accuracy: 0.825333\n",
            "no of neuron= 161 Testing Accuracy: 0.826000\n",
            "no of neuron= 162 Testing Accuracy: 0.834222\n",
            "no of neuron= 163 Testing Accuracy: 0.829333\n",
            "no of neuron= 164 Testing Accuracy: 0.820778\n",
            "no of neuron= 165 Testing Accuracy: 0.831778\n",
            "no of neuron= 166 Testing Accuracy: 0.827889\n",
            "no of neuron= 167 Testing Accuracy: 0.833778\n",
            "no of neuron= 168 Testing Accuracy: 0.832778\n",
            "no of neuron= 169 Testing Accuracy: 0.843000\n",
            "no of neuron= 170 Testing Accuracy: 0.823556\n",
            "no of neuron= 171 Testing Accuracy: 0.836667\n",
            "no of neuron= 172 Testing Accuracy: 0.830889\n",
            "no of neuron= 173 Testing Accuracy: 0.833444\n",
            "no of neuron= 174 Testing Accuracy: 0.843111\n",
            "no of neuron= 175 Testing Accuracy: 0.843444\n",
            "no of neuron= 176 Testing Accuracy: 0.832667\n",
            "no of neuron= 177 Testing Accuracy: 0.832111\n",
            "no of neuron= 178 Testing Accuracy: 0.850333\n",
            "no of neuron= 179 Testing Accuracy: 0.841889\n",
            "no of neuron= 180 Testing Accuracy: 0.862111\n",
            "no of neuron= 181 Testing Accuracy: 0.837667\n",
            "no of neuron= 182 Testing Accuracy: 0.849444\n",
            "no of neuron= 183 Testing Accuracy: 0.844444\n",
            "no of neuron= 184 Testing Accuracy: 0.852667\n",
            "no of neuron= 185 Testing Accuracy: 0.844000\n",
            "no of neuron= 186 Testing Accuracy: 0.850667\n",
            "no of neuron= 187 Testing Accuracy: 0.848778\n",
            "no of neuron= 188 Testing Accuracy: 0.856111\n",
            "no of neuron= 189 Testing Accuracy: 0.849333\n",
            "no of neuron= 190 Testing Accuracy: 0.850111\n",
            "no of neuron= 191 Testing Accuracy: 0.850889\n",
            "no of neuron= 192 Testing Accuracy: 0.859778\n",
            "no of neuron= 193 Testing Accuracy: 0.850778\n",
            "no of neuron= 194 Testing Accuracy: 0.850222\n",
            "no of neuron= 195 Testing Accuracy: 0.856111\n",
            "no of neuron= 196 Testing Accuracy: 0.856889\n",
            "no of neuron= 197 Testing Accuracy: 0.868667\n",
            "no of neuron= 198 Testing Accuracy: 0.865111\n",
            "no of neuron= 199 Testing Accuracy: 0.848000\n",
            "no of neuron= 200 Testing Accuracy: 0.843111\n",
            "no of neuron= 201 Testing Accuracy: 0.861889\n",
            "no of neuron= 202 Testing Accuracy: 0.859889\n",
            "no of neuron= 203 Testing Accuracy: 0.862444\n",
            "no of neuron= 204 Testing Accuracy: 0.848222\n",
            "no of neuron= 205 Testing Accuracy: 0.865778\n",
            "no of neuron= 206 Testing Accuracy: 0.857444\n",
            "no of neuron= 207 Testing Accuracy: 0.854667\n",
            "no of neuron= 208 Testing Accuracy: 0.873111\n",
            "no of neuron= 209 Testing Accuracy: 0.858889\n",
            "no of neuron= 210 Testing Accuracy: 0.866778\n",
            "no of neuron= 211 Testing Accuracy: 0.864889\n",
            "no of neuron= 212 Testing Accuracy: 0.860667\n",
            "no of neuron= 213 Testing Accuracy: 0.863778\n",
            "no of neuron= 214 Testing Accuracy: 0.864222\n",
            "no of neuron= 215 Testing Accuracy: 0.857556\n",
            "no of neuron= 216 Testing Accuracy: 0.865778\n",
            "no of neuron= 217 Testing Accuracy: 0.857556\n",
            "no of neuron= 218 Testing Accuracy: 0.869111\n",
            "no of neuron= 219 Testing Accuracy: 0.871333\n",
            "no of neuron= 220 Testing Accuracy: 0.874556\n",
            "no of neuron= 221 Testing Accuracy: 0.874444\n",
            "no of neuron= 222 Testing Accuracy: 0.866889\n",
            "no of neuron= 223 Testing Accuracy: 0.864778\n",
            "no of neuron= 224 Testing Accuracy: 0.867333\n",
            "no of neuron= 225 Testing Accuracy: 0.875444\n",
            "no of neuron= 226 Testing Accuracy: 0.866889\n",
            "no of neuron= 227 Testing Accuracy: 0.866667\n",
            "no of neuron= 228 Testing Accuracy: 0.866444\n",
            "no of neuron= 229 Testing Accuracy: 0.882667\n",
            "no of neuron= 230 Testing Accuracy: 0.866889\n",
            "no of neuron= 231 Testing Accuracy: 0.877556\n",
            "no of neuron= 232 Testing Accuracy: 0.861111\n",
            "no of neuron= 233 Testing Accuracy: 0.878333\n",
            "no of neuron= 234 Testing Accuracy: 0.870222\n",
            "no of neuron= 235 Testing Accuracy: 0.878889\n",
            "no of neuron= 236 Testing Accuracy: 0.861222\n",
            "no of neuron= 237 Testing Accuracy: 0.867444\n",
            "no of neuron= 238 Testing Accuracy: 0.871556\n",
            "no of neuron= 239 Testing Accuracy: 0.880333\n",
            "no of neuron= 240 Testing Accuracy: 0.871444\n",
            "no of neuron= 241 Testing Accuracy: 0.876667\n",
            "no of neuron= 242 Testing Accuracy: 0.882667\n",
            "no of neuron= 243 Testing Accuracy: 0.877778\n",
            "no of neuron= 244 Testing Accuracy: 0.868556\n",
            "no of neuron= 245 Testing Accuracy: 0.879556\n",
            "no of neuron= 246 Testing Accuracy: 0.884556\n",
            "no of neuron= 247 Testing Accuracy: 0.884333\n",
            "no of neuron= 248 Testing Accuracy: 0.885444\n",
            "no of neuron= 249 Testing Accuracy: 0.886111\n"
          ]
        }
      ],
      "source": [
        "Lmax=250\n",
        "input=x_training_features.size(1)\n",
        "ielm_model = I_ELM(\n",
        "no_input_nodes=input,\n",
        "max_no_hidden_nodes=Lmax,\n",
        "no_output_nodes=no_classes,\n",
        "loss_function=loss_function,\n",
        "activation_function=activation_function,    \n",
        ")\n",
        "activation='s'\n",
        "\n",
        "for i in range(Lmax):\n",
        "    ielm_model.fit(x_training_features,y_train_labels,i,error,activation)\n",
        "    testing_loss, testing_acc = ielm_model.evaluate(x_testing_features, y_testing_labels, metrics=['loss','accuracy'])\n",
        "    #print(\"neurons=\",i,'testing Loss in mean square error: %f ' % testing_loss,i) # loss value\n",
        "    print(\"no of neuron=\",i,'Testing Accuracy: %f' % testing_acc)# accuracy\n",
        "\n",
        "#print('testing Loss in mean square error: %f' % testing_loss) # loss value\n",
        "#print('Testing Accuracy: %f' % testing_acc)# accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 250\n",
        "activation='s'\n",
        "i_elm_model=i_elmTraining(x_training_features,y_train_labels,activation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 300\n",
        "activation='s'\n",
        "\n",
        "i_elm_model1=i_elmTraining(x_training_features,y_train_labels,activation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model1,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 250\n",
        "activation='s'\n",
        "\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 250\n",
        "activation='s'\n",
        "\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 400\n",
        "activation='t'\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 300\n",
        "activation='t'\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 250\n",
        "activation='t'\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 100\n",
        "activation='t'\n",
        "i_elm_model2=i_elmTraining(x_training_features,y_train_labels,activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_elmTesting(i_elm_model2,x_testing_features,y_testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_labels_reshaped=np.argmax(y_train_labels.detach().cpu().numpy(),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_labels_reshaped=np.argmax(y_testing_labels.detach().cpu().numpy(),axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gaussian Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize and train the naive bayes classifier\n",
        "naiveClassifier=GaussianNB(var_smoothing=1e-8)\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "naiveClassifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "\n",
        "print(\"training_time =\", end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred=naiveClassifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,TPR,1]\n",
        "Y=[0,FPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "svm=SVC(C=1,kernel='linear',gamma='auto')\n",
        "\n",
        "start_time= time.time()\n",
        "\n",
        "svm.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred=svm.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,TPR,1]\n",
        "Y=[0,FPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "X=[0,sensitivity,1]\n",
        "Y=[0,specificity,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUC-SCore=\",AUCSCORE)\n",
        "print(\"Specificity:\", specificity,\"sensivity=\",sensitivity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "start_time= time.time()\n",
        "rf_classifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = rf_classifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision tree classifier    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "start_time= time.time()\n",
        "dt_classifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = dt_classifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped,y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "start_time= time.time()\n",
        "dt_classifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = dt_classifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped,y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ET Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "# Create the Extra Trees classifier\n",
        "clf = ExtraTreesClassifier(criterion=\"gini\")\n",
        "\n",
        "start_time=time.time()\n",
        "clf.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "#model.fit(, )\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = clf.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "X=[0,sensitivity,1]\n",
        "Y=[0,specificity,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUC-SCore=\",AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "# Create the Extra Trees classifier\n",
        "clf1 = ExtraTreesClassifier(criterion=\"entropy\")\n",
        "\n",
        "start_time=time.time()\n",
        "clf1.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "#model.fit(, )\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = clf1.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "X=[0,sensitivity,1]\n",
        "Y=[0,specificity,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUC-SCore=\",AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=100,  # Number of boosting rounds\n",
        "    max_depth=3,  # Maximum depth of each tree\n",
        "    learning_rate=0.001,  # Learning rate (step size shrinkage)\n",
        "    objective='multi:softprob',  # Objective function for binary classification\n",
        "    random_state=42,n_jobs=4,booster =\"gbtree\", gamma = 0\n",
        ")\n",
        "start_time=time.time()\n",
        "model.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = model.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "report = classification_report(y_test_labels_reshaped, y_pred)\n",
        "#print(report)\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity,\"sensivity=\",sensitivity)\n",
        "X=[0,sensitivity,1]\n",
        "Y=[0,specificity,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUC-SCore=\",AUCSCORE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "for i in np.arange(8):\n",
        "  # take randomly in indice\n",
        "  ind = random.randint(0, len(x_testing_features))\n",
        "  img = x_testing_features[ind]\n",
        "  # resize the image \n",
        "  img_rs = img.reshape(1,224,224,3)\n",
        "  # predict the label of img\n",
        "  y_pred = ielm_model.predict(img_rs)\n",
        "  # determine the corresponding category\n",
        "  predicted_cate = cate[np.argmax(y_pred)]\n",
        "  plt.subplot(240+1+i)\n",
        "  plt.imshow(img)\n",
        "  plt.title('predicted: ' + str(predicted_cate))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
