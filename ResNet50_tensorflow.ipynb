{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Train IELM and evaluating training accuracy\n",
        "no_classes = 8    #*changed*-----------------------------------\n",
        "error = 0.1\n",
        "loss_function = \"binary_cross_entropy\" #\"mean_squared_error\"  #It can be mean_absolute_error also\n",
        "activation_function = \"tanh\"\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loading_data(train_path,test_path):\n",
        "\n",
        "    # Define the data transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images to (224, 224)\n",
        "        transforms.ToTensor(),  # Convert images to tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
        "    ])\n",
        "\n",
        "    \n",
        "\n",
        "    train_dataset = ImageFolder(train_path, transform=transform)\n",
        "    traindata_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_dataset = ImageFolder(test_path, transform=transform)\n",
        "    testdata_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "    return traindata_loader,testdata_loader\n",
        "\n",
        "def load_model():\n",
        "    #loading of model\n",
        "    device=torch.device(\"cuda\")  \n",
        "    \n",
        "    base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    base_model = torch.nn.Sequential(*list(base_model.children())[:-1])\n",
        "    base_model=base_model.to(device)\n",
        "    base_model.eval()\n",
        "    return base_model\n",
        "#training features extraction\n",
        "def training_feature_extraction(device, traindata_loader,resnet_model):\n",
        "\n",
        "    x_training_features = []\n",
        "    y_train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in traindata_loader:        \n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Extract features using VGG16\n",
        "            with torch.no_grad():\n",
        "                features = resnet_model(images)   \n",
        "            \n",
        "            x_training_features.append(features)\n",
        "            y_train_labels.append(labels)\n",
        "    x_training_features = torch.cat(x_training_features, dim=0)\n",
        "    y_train_labels = torch.cat(y_train_labels, dim=0)\n",
        "\n",
        "    x_training_features.to(device)\n",
        "    y_train_labels.to(device)\n",
        "    return x_training_features,y_train_labels\n",
        "\n",
        "def testing_feature_extratcion(device,testdata_loader,vgg_model):\n",
        "    x_testing_features = []\n",
        "    y_testing_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testdata_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Extract features using VGG19\n",
        "            with torch.no_grad():\n",
        "                features = vgg_model(images) \n",
        "            x_testing_features.append(vgg_model(images))\n",
        "            y_testing_labels.append(labels)\n",
        "    x_testing_features = torch.cat(x_testing_features, dim=0)\n",
        "    y_testing_labels = torch.cat(y_testing_labels, dim=0)\n",
        "\n",
        "    x_testing_features.to(device)\n",
        "    y_testing_labels.to(device)\n",
        "    return x_testing_features,y_testing_labels\n",
        "\n",
        "\"\"\"\n",
        "IELM Constructor class\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "class I_ELM():\n",
        "    \"\"\" Constructor to initialize node\"\"\"\n",
        "    def __init__(self, no_input_nodes, max_no_hidden_nodes, no_output_nodes,\n",
        "        activation_function='sigmoid', loss_function='mean_squared_error'):\n",
        "        device=torch.device(\"cuda\") \n",
        "        \n",
        "        self.no_input_nodes = no_input_nodes\n",
        "        self.no_hidden_nodes = 1\n",
        "        self.no_output_nodes = no_output_nodes\n",
        "\n",
        "        self.beta = torch.FloatTensor(self.no_hidden_nodes, self.no_output_nodes).uniform_(-1., 1.)\n",
        "        self.beta=self.beta.to(device)\n",
        "        \n",
        "        # initialize weights between Input Layer and hidden layer\n",
        "        self.alpha=torch.FloatTensor(self.no_input_nodes, self.no_hidden_nodes).uniform_(-1.,1.)\n",
        "        self.alpha=self.alpha.to(device)\n",
        "        \n",
        "        # Initialize Biases\n",
        "        self.bias =torch.zeros(size=(self.no_hidden_nodes,))\n",
        "        self.bias=self.bias.to(device)\n",
        "        # set an activation function\n",
        "        self.activation_function = activation_function\n",
        "       \n",
        "        # set a loss function\n",
        "        self.loss_function = loss_function\n",
        "    \n",
        "    def mean_squared_error(self,Y_True, Y_Pred):\n",
        "        return 0.5 * torch.mean((Y_True - Y_Pred)**2)\n",
        "\n",
        "    def mean_absolute_error(self, Y_True, Y_Pred):\n",
        "        return torch.mean(torch.abs(Y_True - Y_Pred))\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1. /(1.+ torch.exp(-x))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return torch.tensor(self(X), dtype=torch.float32)\n",
        "    \n",
        "    def __call__(self, X):\n",
        "        h = torch.sigmoid(torch.matmul(X,self.alpha) + self.bias)\n",
        "        return torch.matmul(h,self.beta)\n",
        "    \n",
        "    def binary_cross_entropy(self,Y_true, Y_pred):\n",
        "        lss=[]\n",
        "        Y_pred = torch.clamp(Y_pred, min=1e-7, max=1.0 - 1e-7)\n",
        "        loss_calculated=- (Y_true * torch.log(Y_pred) + (1 - Y_true) * torch.log(1 - Y_pred))\n",
        "        '''loss_calculated_max=torch.argmax(loss_calculated, axis=-1)\n",
        "        loss_calculated_max=loss_calculated_max.cpu()\n",
        "        \n",
        "        for i in range(len(loss_calculated_max)):\n",
        "            l1=loss_calculated_max[i]\n",
        "            lss.append(l1)\n",
        "        #print(lss)\n",
        "        plt.plot(range(1, Lmax), lss)\n",
        "        plt.xlabel('Hidden Layer Neurons')\n",
        "        plt.ylabel('Training Loss')\n",
        "        #plt.title('BCE Loss Over Epochs')\n",
        "        plt.show()\n",
        "        '''\n",
        "        loss=torch.mean(loss_calculated)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def plot_confusion(self,Y_true, Y_pred):\n",
        "         \n",
        "        # Calculate the confusion matrix \n",
        "        confusion = confusion_matrix(Y_true, Y_pred)\n",
        "        \n",
        "        # Extract values from the confusion matrix\n",
        "        TP = confusion[1, 1]\n",
        "        FP = confusion[0, 1]\n",
        "        TN = confusion[0, 0]\n",
        "        FN = confusion[1, 0]\n",
        "        TPR= TP/(TP+FN)\n",
        "        FPR=FP/(FP+TN)\n",
        "        X=[0,FPR,1]\n",
        "        Y=[0,TPR,1]\n",
        "        AUCSCORE =np.trapz(Y,X)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score\n",
        "        precision, recall, f1_score, _ = precision_recall_fscore_support(Y_true,Y_pred, average='macro')\n",
        "\n",
        "        print(\"precision=\",precision,\"recall=\",recall,\"f1-score=\",f1_score,\"auc-score=\",AUCSCORE)\n",
        "\n",
        "        Sensitivity = confusion[0][0]/(confusion[0][0]+confusion[1][1])\n",
        "        Specificity =confusion[1][1]/(confusion[0][1]+confusion[1][1])\n",
        "        fpr = 1-Specificity\n",
        "        print(\"sensitivity=\",Sensitivity,\"specificity=\",Specificity,\"fpr=\",fpr)\n",
        "        # Print the confusion matrix\n",
        "        print(\"Confusion Matrix:\",confusion)\n",
        "         \n",
        "        import pandas as pd\n",
        "\n",
        "        data = {'Column1':[precision,recall,f1_score,AUCSCORE,Sensitivity,Specificity,fpr]}\n",
        "        df_matric = pd.DataFrame(data=data)\n",
        "        df_matric.index  = ['Precision', 'recall', 'f1-score','Accuracy Score','sensitivity','specificity','fpr']\n",
        "        print(df_matric)\n",
        "\n",
        "        \n",
        "        # Get the number of classes\n",
        "        num_classes = len(np.unique(Y_true))\n",
        "\n",
        "        # Create a figure and axis\n",
        "        # Create a figure and axis\n",
        "        fig, ax = plt.subplots(figsize=(4,4))\n",
        "        cmlabels=['Apple','Corn','Grape','Potato','Rice','Tea','Tomato','Wheat']\n",
        "        # Plot the confusion matrix using a heatmap\n",
        "        sns.heatmap(confusion, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, ax=ax,xticklabels=cmlabels,yticklabels=cmlabels)\n",
        "\n",
        "        # Set axis labels and title\n",
        "        ax.set_xlabel(\"Predicted labels\")\n",
        "        ax.set_ylabel(\"Actual labels\")\n",
        "        title=\"CM For testing on Healthy Plants RESNET 50 AND TF\"\n",
        "        ax.set_title(\"\")\n",
        "        plt.savefig(title+'confusion_matrix.png')\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, X, Y_true, metrics):\n",
        "        Y_pred =self.predict(X)\n",
        "        #Y_true = Y_true\n",
        "        \n",
        "        ret = []\n",
        "        \n",
        "        Y_pred=torch.tensor(Y_pred,dtype=torch.float32)\n",
        "\n",
        "\n",
        "        for m in metrics:\n",
        "            if m == 'loss':\n",
        "                loss=[]\n",
        "                loss = self.binary_cross_entropy(Y_true, Y_pred)\n",
        "                ret.append(loss)                \n",
        "            elif m == 'accuracy':\n",
        "                Y_pred_argmax = torch.argmax(Y_pred, axis=-1)\n",
        "                Y_true_argmax = torch.argmax(Y_true, axis=-1)\n",
        "                acc = torch.sum(Y_pred_argmax == Y_true_argmax) / len(Y_true)\n",
        "                #plotting of confusion matrix\n",
        "\n",
        "                # Get predicted probabilities for positive class\n",
        "                Y_true_argmax=Y_true_argmax.cpu()\n",
        "                Y_pred_argmax=Y_pred_argmax.cpu()\n",
        "                #self.plot_confusion(Y_true_argmax, Y_pred_argmax)\n",
        "                ret.append(acc)\n",
        "            else:\n",
        "                raise ValueError('an unknown evaluation indicator \\'%s\\'.' % m)\n",
        "        if len(ret) == 1:\n",
        "            print(ret)\n",
        "            ret = ret\n",
        "        elif len(ret) == 0:\n",
        "            ret = None\n",
        "        return ret\n",
        "\n",
        "    def fit(self, X, Y_true,Lmax,error,activation):\n",
        "        device=torch.device(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            self.beta=torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "            self.alpha = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "            #print(self.beta.shape, self.alpha.shape,X.shape)\n",
        "            \n",
        "            self.alpha=self.alpha.to(device)\n",
        "            self.beta=self.beta.to(device)\n",
        "            if(activation=='s'):\n",
        "                H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "            elif(activation=='t'):\n",
        "                H = torch.tanh(torch.matmul(X,self.alpha)) \n",
        "            elif(activation=='r'):\n",
        "                H = torch.relu(torch.matmul(X, self.alpha))\n",
        "            \n",
        "            # Compute a pseudoinverse of H\n",
        "            H_pinv = torch.pinverse(H)\n",
        "\n",
        "            # Update beta\n",
        "            self.beta = torch.matmul(H_pinv, Y_true.float())\n",
        "            \n",
        "            for i in range(2, Lmax):\n",
        "                beta_random = torch.FloatTensor(np.random.uniform(-1.,1.,size=(1, self.no_output_nodes)))\n",
        "                alpha_random = torch.FloatTensor(self.no_input_nodes, 1).uniform_(-1,1)\n",
        "                \n",
        "                beta_random=beta_random.to(device)\n",
        "                alpha_random=alpha_random.to(device)\n",
        "                \n",
        "                self.alpha=torch.cat((self.alpha,alpha_random),dim=1)                          \n",
        "                self.beta =torch.cat((self.beta,beta_random),dim=0)\n",
        "\n",
        "                if(activation=='s'):\n",
        "                    H = torch.sigmoid(torch.matmul(X, self.alpha))\n",
        "                elif(activation=='t'):\n",
        "                    H = torch.tanh(torch.matmul(X, self.alpha))  \n",
        "                elif(activation=='r'):\n",
        "                    H = torch.relu(torch.matmul(X, self.alpha))\n",
        "                \n",
        "                H_pinv = torch.pinverse(H)\n",
        "                self.beta = torch.matmul(H_pinv, Y_true) \n",
        "\n",
        "from  keras.utils import np_utils\n",
        "def train_features_conversion(x_training_features,y_train_labels):\n",
        "\n",
        "    # Convert the extracted features to numpy arrays\n",
        "\n",
        "    device=torch.device(\"cuda\")\n",
        "    x_training_features_np = x_training_features.detach().cpu().numpy()  \n",
        "    flattened_features_train = x_training_features_np.reshape(x_training_features_np.shape[0], -1)\n",
        "    train_features_tensor = torch.tensor(flattened_features_train, dtype=torch.float32).to(device)\n",
        "    \n",
        "    y_train_array= y_train_labels.cpu().detach().numpy()\n",
        "    y_train_array_encoded = np_utils.to_categorical(y_train_array, no_classes) #changes\n",
        "    y_train_labels_encoded=torch.tensor(y_train_array_encoded,dtype=torch.float32)\n",
        "    y_train_labels_encoded=y_train_labels_encoded.to(device)    \n",
        "    return train_features_tensor,y_train_labels_encoded\n",
        "\n",
        "def test_features_conversion(x_testing_features,y_testing_labels):\n",
        "    # Convert the extracted features to numpy arrays\n",
        "    device=torch.device(\"cuda\")\n",
        "    x_testing_features_np = x_testing_features.detach().cpu().numpy()\n",
        "    \n",
        "    flattened_features_test = x_testing_features_np.reshape(x_testing_features_np.shape[0], -1)\n",
        "    test_features_tensor = torch.tensor(flattened_features_test,dtype=torch.float32).to(device)\n",
        "    \n",
        "    y_test_array= y_testing_labels.cpu().detach().numpy()\n",
        "    y_test_array_encoded = np_utils.to_categorical(y_test_array, no_classes) #changes\n",
        "    y_test_labels_encoded=torch.tensor(y_test_array_encoded,dtype=torch.float32)\n",
        "    y_test_labels_encoded=y_test_labels_encoded.to(device)\n",
        "    return test_features_tensor, y_test_labels_encoded\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\") \n",
        "train_path=\"../Plant Health Classification/Dataset/Type_of_plant/Train/\"\n",
        "test_path=\"../Plant Health Classification/Dataset/Type_of_plant/Test/\"\n",
        "traindata_loader,testdata_loader=loading_data(train_path,test_path)\n",
        "model=load_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_training_features,y_train_labels=training_feature_extraction(device,traindata_loader,model)\n",
        "train_features_tensor,y_train_labels_encoded=train_features_conversion(x_training_features,y_train_labels)\n",
        "torch.save(train_features_tensor,'../Plant Health Classification/Feautres_Extraction/Resnet50/Train/train_features.pth')\n",
        "torch.save(y_train_labels_encoded,\"../Plant Health Classification2/Feautres_Extraction/Resnet50/Train/train_labels.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_testing_features,y_testing_labels=testing_feature_extratcion(device,testdata_loader,model)\n",
        "test_features_tensor, y_test_labels_encoded=test_features_conversion(x_testing_features,y_testing_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(test_features_tensor,\"../Plant Health Classification/Feautres_Extraction/Resnet50/Test/testing_features.pth\")\n",
        "torch.save(y_test_labels_encoded,\"../Plant Health Classification/Feautres_Extraction/Resnet50/Test/testing_labels.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def i_elmTraining(features_train,y_train,activation):\n",
        "  # ===============================\n",
        "  # Instantiate ELM object\n",
        "  # ===============================\n",
        "  input=features_train.size(1) #---------------------------------#changed\n",
        "  #print(features_train.size)   CHANGED HERE UNNECESSARY\n",
        "  \n",
        "  ielm_model = I_ELM(\n",
        "      no_input_nodes=input,\n",
        "      max_no_hidden_nodes=Lmax,\n",
        "      no_output_nodes=no_classes,\n",
        "      loss_function=loss_function,\n",
        "      activation_function=activation_function,    \n",
        "  )\n",
        "\n",
        "\n",
        "  i = time.time()  \n",
        "  ielm_model.fit(features_train,y_train,Lmax,error,activation)\n",
        "  final = time.time()\n",
        "\n",
        "\n",
        "  '''tf.keras.saving.save_model(\n",
        "   ielm_model, 'D:/Smart_Farming_Capstone_Project_G201/Model02/model02RESNET.h5', overwrite=True, save_format='h5')'''\n",
        "  #tf.keras.models.save_model(filepath='D:/Smart_Farming_Capstone_Project_G201/Model02/')\n",
        "  training_loss, training_acc = ielm_model.evaluate(features_train, y_train, metrics=['loss','accuracy'])\n",
        "  \n",
        "  #ielm_model.save(\"modelResnet.h5\") #CHANGES -----------------------------------------\n",
        "  print('Training Loss in mean square error: %f' % training_loss) # loss value\n",
        "  print('Training Accuracy: %f' % training_acc)# accuracy\n",
        "  print('Total Time require for Training %f Seconds'% (final-i))\n",
        "\n",
        "  return ielm_model\n",
        "def i_elmTesting(ielm_model,features_test,y_test):\n",
        "  \n",
        "  i = time.time()\n",
        "  test_loss, test_acc = ielm_model.evaluate(features_test, y_test, metrics=['loss', 'accuracy'])\n",
        "  final = time.time()\n",
        "  \n",
        "  print('Testing Loss in mean square error: %f ' % test_loss,\"\\n\")\n",
        "  print('Testing Accuracy: %f' % test_acc)\n",
        "  print(\"Testing Time is \" ,(final-i))\n",
        "  time_taken=(final-i)/features_test.shape[0]\n",
        "  print('Total Time require for Testing one image is ' ,time_taken ,'seconds')\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**CHANGES STARTED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_training_features=torch.load(\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/Resnet50/Train/train_features.pth\")\n",
        "y_train_labels=torch.load(\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/Resnet50/Train/train_labels.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x_testing_features=torch.load(\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/Resnet50/Test/testing_features.pth\")\n",
        "y_testing_labels=torch.load(\"D:/Smart_Farming_Capstone_Project_G201/Model02/Feautres_Extraction/Resnet50/Test/testing_labels.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**CHANGES ENDED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dr. Rajendra Ku Roul\\AppData\\Local\\Temp\\ipykernel_1400\\407467188.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(self(X), dtype=torch.float32)\n",
            "C:\\Users\\Dr. Rajendra Ku Roul\\AppData\\Local\\Temp\\ipykernel_1400\\407467188.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_pred=torch.tensor(Y_pred,dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no of neuron= 0 Testing Accuracy: 0.125000\n",
            "no of neuron= 1 Testing Accuracy: 0.125000\n",
            "no of neuron= 2 Testing Accuracy: 0.125000\n",
            "no of neuron= 3 Testing Accuracy: 0.125000\n",
            "no of neuron= 4 Testing Accuracy: 0.258111\n",
            "no of neuron= 5 Testing Accuracy: 0.281111\n",
            "no of neuron= 6 Testing Accuracy: 0.227667\n",
            "no of neuron= 7 Testing Accuracy: 0.340111\n",
            "no of neuron= 8 Testing Accuracy: 0.141222\n",
            "no of neuron= 9 Testing Accuracy: 0.241222\n",
            "no of neuron= 10 Testing Accuracy: 0.239222\n",
            "no of neuron= 11 Testing Accuracy: 0.393111\n",
            "no of neuron= 12 Testing Accuracy: 0.312889\n",
            "no of neuron= 13 Testing Accuracy: 0.329333\n",
            "no of neuron= 14 Testing Accuracy: 0.275000\n",
            "no of neuron= 15 Testing Accuracy: 0.378556\n",
            "no of neuron= 16 Testing Accuracy: 0.455556\n",
            "no of neuron= 17 Testing Accuracy: 0.531333\n",
            "no of neuron= 18 Testing Accuracy: 0.395000\n",
            "no of neuron= 19 Testing Accuracy: 0.488444\n",
            "no of neuron= 20 Testing Accuracy: 0.154000\n",
            "no of neuron= 21 Testing Accuracy: 0.457889\n",
            "no of neuron= 22 Testing Accuracy: 0.410667\n",
            "no of neuron= 23 Testing Accuracy: 0.447778\n",
            "no of neuron= 24 Testing Accuracy: 0.492000\n",
            "no of neuron= 25 Testing Accuracy: 0.431889\n",
            "no of neuron= 26 Testing Accuracy: 0.474556\n",
            "no of neuron= 27 Testing Accuracy: 0.651556\n",
            "no of neuron= 28 Testing Accuracy: 0.454333\n",
            "no of neuron= 29 Testing Accuracy: 0.640111\n",
            "no of neuron= 30 Testing Accuracy: 0.451556\n",
            "no of neuron= 31 Testing Accuracy: 0.557000\n",
            "no of neuron= 32 Testing Accuracy: 0.471889\n",
            "no of neuron= 33 Testing Accuracy: 0.531778\n",
            "no of neuron= 34 Testing Accuracy: 0.548889\n",
            "no of neuron= 35 Testing Accuracy: 0.697222\n",
            "no of neuron= 36 Testing Accuracy: 0.657111\n",
            "no of neuron= 37 Testing Accuracy: 0.317222\n",
            "no of neuron= 38 Testing Accuracy: 0.517111\n",
            "no of neuron= 39 Testing Accuracy: 0.516667\n",
            "no of neuron= 40 Testing Accuracy: 0.614444\n",
            "no of neuron= 41 Testing Accuracy: 0.684889\n",
            "no of neuron= 42 Testing Accuracy: 0.653444\n",
            "no of neuron= 43 Testing Accuracy: 0.758111\n",
            "no of neuron= 44 Testing Accuracy: 0.608556\n",
            "no of neuron= 45 Testing Accuracy: 0.671778\n",
            "no of neuron= 46 Testing Accuracy: 0.725444\n",
            "no of neuron= 47 Testing Accuracy: 0.833222\n",
            "no of neuron= 48 Testing Accuracy: 0.662556\n",
            "no of neuron= 49 Testing Accuracy: 0.534111\n",
            "no of neuron= 50 Testing Accuracy: 0.753889\n",
            "no of neuron= 51 Testing Accuracy: 0.661333\n",
            "no of neuron= 52 Testing Accuracy: 0.780000\n",
            "no of neuron= 53 Testing Accuracy: 0.653778\n",
            "no of neuron= 54 Testing Accuracy: 0.728333\n",
            "no of neuron= 55 Testing Accuracy: 0.522778\n",
            "no of neuron= 56 Testing Accuracy: 0.633556\n",
            "no of neuron= 57 Testing Accuracy: 0.577667\n",
            "no of neuron= 58 Testing Accuracy: 0.535889\n",
            "no of neuron= 59 Testing Accuracy: 0.803222\n",
            "no of neuron= 60 Testing Accuracy: 0.765667\n",
            "no of neuron= 61 Testing Accuracy: 0.698556\n",
            "no of neuron= 62 Testing Accuracy: 0.535889\n",
            "no of neuron= 63 Testing Accuracy: 0.715333\n",
            "no of neuron= 64 Testing Accuracy: 0.771333\n",
            "no of neuron= 65 Testing Accuracy: 0.729667\n",
            "no of neuron= 66 Testing Accuracy: 0.648556\n",
            "no of neuron= 67 Testing Accuracy: 0.801667\n",
            "no of neuron= 68 Testing Accuracy: 0.755444\n",
            "no of neuron= 69 Testing Accuracy: 0.760333\n",
            "no of neuron= 70 Testing Accuracy: 0.697000\n",
            "no of neuron= 71 Testing Accuracy: 0.709778\n",
            "no of neuron= 72 Testing Accuracy: 0.799111\n",
            "no of neuron= 73 Testing Accuracy: 0.687556\n",
            "no of neuron= 74 Testing Accuracy: 0.687333\n",
            "no of neuron= 75 Testing Accuracy: 0.495111\n",
            "no of neuron= 76 Testing Accuracy: 0.759111\n",
            "no of neuron= 77 Testing Accuracy: 0.716778\n",
            "no of neuron= 78 Testing Accuracy: 0.771222\n",
            "no of neuron= 79 Testing Accuracy: 0.714333\n",
            "no of neuron= 80 Testing Accuracy: 0.739556\n",
            "no of neuron= 81 Testing Accuracy: 0.724778\n",
            "no of neuron= 82 Testing Accuracy: 0.741333\n",
            "no of neuron= 83 Testing Accuracy: 0.659000\n",
            "no of neuron= 84 Testing Accuracy: 0.853111\n",
            "no of neuron= 85 Testing Accuracy: 0.633667\n",
            "no of neuron= 86 Testing Accuracy: 0.742333\n",
            "no of neuron= 87 Testing Accuracy: 0.863333\n",
            "no of neuron= 88 Testing Accuracy: 0.651444\n",
            "no of neuron= 89 Testing Accuracy: 0.780444\n",
            "no of neuron= 90 Testing Accuracy: 0.749778\n",
            "no of neuron= 91 Testing Accuracy: 0.847111\n",
            "no of neuron= 92 Testing Accuracy: 0.767222\n",
            "no of neuron= 93 Testing Accuracy: 0.817111\n",
            "no of neuron= 94 Testing Accuracy: 0.685889\n",
            "no of neuron= 95 Testing Accuracy: 0.797222\n",
            "no of neuron= 96 Testing Accuracy: 0.772222\n",
            "no of neuron= 97 Testing Accuracy: 0.844333\n",
            "no of neuron= 98 Testing Accuracy: 0.672333\n",
            "no of neuron= 99 Testing Accuracy: 0.738889\n",
            "no of neuron= 100 Testing Accuracy: 0.783222\n",
            "no of neuron= 101 Testing Accuracy: 0.764889\n",
            "no of neuron= 102 Testing Accuracy: 0.708000\n",
            "no of neuron= 103 Testing Accuracy: 0.744444\n",
            "no of neuron= 104 Testing Accuracy: 0.838222\n",
            "no of neuron= 105 Testing Accuracy: 0.865556\n",
            "no of neuron= 106 Testing Accuracy: 0.605000\n",
            "no of neuron= 107 Testing Accuracy: 0.526111\n",
            "no of neuron= 108 Testing Accuracy: 0.823556\n",
            "no of neuron= 109 Testing Accuracy: 0.801111\n",
            "no of neuron= 110 Testing Accuracy: 0.758111\n",
            "no of neuron= 111 Testing Accuracy: 0.579444\n",
            "no of neuron= 112 Testing Accuracy: 0.756000\n",
            "no of neuron= 113 Testing Accuracy: 0.702778\n",
            "no of neuron= 114 Testing Accuracy: 0.796000\n",
            "no of neuron= 115 Testing Accuracy: 0.874000\n",
            "no of neuron= 116 Testing Accuracy: 0.758667\n",
            "no of neuron= 117 Testing Accuracy: 0.883333\n",
            "no of neuron= 118 Testing Accuracy: 0.818000\n",
            "no of neuron= 119 Testing Accuracy: 0.736222\n",
            "no of neuron= 120 Testing Accuracy: 0.870222\n",
            "no of neuron= 121 Testing Accuracy: 0.674889\n",
            "no of neuron= 122 Testing Accuracy: 0.942000\n",
            "no of neuron= 123 Testing Accuracy: 0.628556\n",
            "no of neuron= 124 Testing Accuracy: 0.886222\n",
            "no of neuron= 125 Testing Accuracy: 0.894444\n",
            "no of neuron= 126 Testing Accuracy: 0.770889\n",
            "no of neuron= 127 Testing Accuracy: 0.919333\n",
            "no of neuron= 128 Testing Accuracy: 0.855556\n",
            "no of neuron= 129 Testing Accuracy: 0.933222\n",
            "no of neuron= 130 Testing Accuracy: 0.880000\n",
            "no of neuron= 131 Testing Accuracy: 0.654889\n",
            "no of neuron= 132 Testing Accuracy: 0.722333\n",
            "no of neuron= 133 Testing Accuracy: 0.888000\n",
            "no of neuron= 134 Testing Accuracy: 0.854556\n",
            "no of neuron= 135 Testing Accuracy: 0.894333\n",
            "no of neuron= 136 Testing Accuracy: 0.749667\n",
            "no of neuron= 137 Testing Accuracy: 0.827778\n",
            "no of neuron= 138 Testing Accuracy: 0.837667\n",
            "no of neuron= 139 Testing Accuracy: 0.616889\n",
            "no of neuron= 140 Testing Accuracy: 0.484889\n",
            "no of neuron= 141 Testing Accuracy: 0.887000\n",
            "no of neuron= 142 Testing Accuracy: 0.912111\n",
            "no of neuron= 143 Testing Accuracy: 0.917333\n",
            "no of neuron= 144 Testing Accuracy: 0.856778\n",
            "no of neuron= 145 Testing Accuracy: 0.848667\n",
            "no of neuron= 146 Testing Accuracy: 0.688222\n",
            "no of neuron= 147 Testing Accuracy: 0.755333\n",
            "no of neuron= 148 Testing Accuracy: 0.913556\n",
            "no of neuron= 149 Testing Accuracy: 0.841667\n",
            "no of neuron= 150 Testing Accuracy: 0.862222\n",
            "no of neuron= 151 Testing Accuracy: 0.661333\n",
            "no of neuron= 152 Testing Accuracy: 0.801778\n",
            "no of neuron= 153 Testing Accuracy: 0.741222\n",
            "no of neuron= 154 Testing Accuracy: 0.862778\n",
            "no of neuron= 155 Testing Accuracy: 0.710222\n",
            "no of neuron= 156 Testing Accuracy: 0.865889\n",
            "no of neuron= 157 Testing Accuracy: 0.790667\n",
            "no of neuron= 158 Testing Accuracy: 0.869778\n",
            "no of neuron= 159 Testing Accuracy: 0.942444\n",
            "no of neuron= 160 Testing Accuracy: 0.929111\n",
            "no of neuron= 161 Testing Accuracy: 0.697556\n",
            "no of neuron= 162 Testing Accuracy: 0.942778\n",
            "no of neuron= 163 Testing Accuracy: 0.926444\n",
            "no of neuron= 164 Testing Accuracy: 0.762444\n",
            "no of neuron= 165 Testing Accuracy: 0.824444\n",
            "no of neuron= 166 Testing Accuracy: 0.867778\n",
            "no of neuron= 167 Testing Accuracy: 0.864333\n",
            "no of neuron= 168 Testing Accuracy: 0.862667\n",
            "no of neuron= 169 Testing Accuracy: 0.943667\n",
            "no of neuron= 170 Testing Accuracy: 0.827000\n",
            "no of neuron= 171 Testing Accuracy: 0.850667\n",
            "no of neuron= 172 Testing Accuracy: 0.767000\n",
            "no of neuron= 173 Testing Accuracy: 0.913222\n",
            "no of neuron= 174 Testing Accuracy: 0.886333\n",
            "no of neuron= 175 Testing Accuracy: 0.834222\n",
            "no of neuron= 176 Testing Accuracy: 0.839222\n",
            "no of neuron= 177 Testing Accuracy: 0.891000\n",
            "no of neuron= 178 Testing Accuracy: 0.738667\n",
            "no of neuron= 179 Testing Accuracy: 0.797000\n",
            "no of neuron= 180 Testing Accuracy: 0.897778\n",
            "no of neuron= 181 Testing Accuracy: 0.907444\n",
            "no of neuron= 182 Testing Accuracy: 0.669222\n",
            "no of neuron= 183 Testing Accuracy: 0.801556\n",
            "no of neuron= 184 Testing Accuracy: 0.874111\n",
            "no of neuron= 185 Testing Accuracy: 0.902667\n",
            "no of neuron= 186 Testing Accuracy: 0.937333\n",
            "no of neuron= 187 Testing Accuracy: 0.866556\n",
            "no of neuron= 188 Testing Accuracy: 0.717222\n",
            "no of neuron= 189 Testing Accuracy: 0.891333\n",
            "no of neuron= 190 Testing Accuracy: 0.925889\n",
            "no of neuron= 191 Testing Accuracy: 0.716000\n",
            "no of neuron= 192 Testing Accuracy: 0.943889\n",
            "no of neuron= 193 Testing Accuracy: 0.885667\n",
            "no of neuron= 194 Testing Accuracy: 0.944778\n",
            "no of neuron= 195 Testing Accuracy: 0.656778\n",
            "no of neuron= 196 Testing Accuracy: 0.817889\n",
            "no of neuron= 197 Testing Accuracy: 0.920667\n",
            "no of neuron= 198 Testing Accuracy: 0.802667\n",
            "no of neuron= 199 Testing Accuracy: 0.908667\n",
            "no of neuron= 200 Testing Accuracy: 0.931778\n",
            "no of neuron= 201 Testing Accuracy: 0.905111\n",
            "no of neuron= 202 Testing Accuracy: 0.943000\n",
            "no of neuron= 203 Testing Accuracy: 0.792556\n",
            "no of neuron= 204 Testing Accuracy: 0.947000\n",
            "no of neuron= 205 Testing Accuracy: 0.740000\n",
            "no of neuron= 206 Testing Accuracy: 0.916333\n",
            "no of neuron= 207 Testing Accuracy: 0.928889\n",
            "no of neuron= 208 Testing Accuracy: 0.779556\n",
            "no of neuron= 209 Testing Accuracy: 0.888667\n",
            "no of neuron= 210 Testing Accuracy: 0.847333\n",
            "no of neuron= 211 Testing Accuracy: 0.661444\n",
            "no of neuron= 212 Testing Accuracy: 0.930111\n",
            "no of neuron= 213 Testing Accuracy: 0.740778\n",
            "no of neuron= 214 Testing Accuracy: 0.770222\n",
            "no of neuron= 215 Testing Accuracy: 0.887556\n",
            "no of neuron= 216 Testing Accuracy: 0.703111\n",
            "no of neuron= 217 Testing Accuracy: 0.889444\n",
            "no of neuron= 218 Testing Accuracy: 0.841778\n",
            "no of neuron= 219 Testing Accuracy: 0.846778\n",
            "no of neuron= 220 Testing Accuracy: 0.907556\n",
            "no of neuron= 221 Testing Accuracy: 0.829111\n",
            "no of neuron= 222 Testing Accuracy: 0.813556\n",
            "no of neuron= 223 Testing Accuracy: 0.931333\n",
            "no of neuron= 224 Testing Accuracy: 0.775667\n",
            "no of neuron= 225 Testing Accuracy: 0.598667\n",
            "no of neuron= 226 Testing Accuracy: 0.929889\n",
            "no of neuron= 227 Testing Accuracy: 0.908444\n",
            "no of neuron= 228 Testing Accuracy: 0.877111\n",
            "no of neuron= 229 Testing Accuracy: 0.921111\n",
            "no of neuron= 230 Testing Accuracy: 0.877222\n",
            "no of neuron= 231 Testing Accuracy: 0.909111\n",
            "no of neuron= 232 Testing Accuracy: 0.826111\n",
            "no of neuron= 233 Testing Accuracy: 0.875444\n",
            "no of neuron= 234 Testing Accuracy: 0.859222\n",
            "no of neuron= 235 Testing Accuracy: 0.825889\n",
            "no of neuron= 236 Testing Accuracy: 0.805000\n",
            "no of neuron= 237 Testing Accuracy: 0.915778\n",
            "no of neuron= 238 Testing Accuracy: 0.920667\n",
            "no of neuron= 239 Testing Accuracy: 0.845778\n",
            "no of neuron= 240 Testing Accuracy: 0.881111\n",
            "no of neuron= 241 Testing Accuracy: 0.918556\n",
            "no of neuron= 242 Testing Accuracy: 0.910222\n",
            "no of neuron= 243 Testing Accuracy: 0.941222\n",
            "no of neuron= 244 Testing Accuracy: 0.909333\n",
            "no of neuron= 245 Testing Accuracy: 0.936333\n",
            "no of neuron= 246 Testing Accuracy: 0.942667\n",
            "no of neuron= 247 Testing Accuracy: 0.909222\n",
            "no of neuron= 248 Testing Accuracy: 0.889333\n",
            "no of neuron= 249 Testing Accuracy: 0.952333\n"
          ]
        }
      ],
      "source": [
        "Lmax=250\n",
        "input=x_training_features.size(1)\n",
        "ielm_model = I_ELM(\n",
        "no_input_nodes=input,\n",
        "max_no_hidden_nodes=Lmax,\n",
        "no_output_nodes=no_classes,\n",
        "loss_function=loss_function,\n",
        "activation_function=activation_function,    \n",
        ")\n",
        "activation='t'\n",
        "\n",
        "for i in range(Lmax):\n",
        "    ielm_model.fit(x_training_features,y_train_labels,i,error,activation)\n",
        "    testing_loss, testing_acc = ielm_model.evaluate(x_testing_features, y_testing_labels, metrics=['loss','accuracy'])\n",
        "    #print(\"neurons=\",i,'testing Loss in mean square error: %f ' % testing_loss,i) # loss value\n",
        "    print(\"no of neuron=\",i,'Testing Accuracy: %f' % testing_acc)# accuracy\n",
        "\n",
        "#print('testing Loss in mean square error: %f' % testing_loss) # loss value\n",
        "#print('Testing Accuracy: %f' % testing_acc)# accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Lmax= 100\n",
        "activation='s'\n",
        "i_elm_model=i_elmTraining(x_training_features,y_train_labels,activation)\n",
        "i_elmTesting(ielm_model,x_testing_features,y_testing_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_labels_reshaped=np.argmax(y_train_labels.detach().cpu().numpy(),axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_labels_reshaped=np.argmax(y_testing_labels.detach().cpu().numpy(),axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing of ML classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize and train the naive bayes classifier\n",
        "naiveClassifier=GaussianNB(var_smoothing=1e-8)\n",
        "\n",
        "start_time=time.time()\n",
        "naiveClassifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "\n",
        "print(\"training_time =\", end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred=naiveClassifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "svm=SVC(C=1,kernel='linear',gamma='auto')\n",
        "\n",
        "start_time= time.time()\n",
        "#svm=svm.cuda().to(device)\n",
        "svm.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred=svm.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity,\"sensivity=\",sensitivity)\n",
        "\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "start_time= time.time()\n",
        "rf_classifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = rf_classifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision tree classifier    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "start_time= time.time()\n",
        "dt_classifier.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = dt_classifier.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped,y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier1 = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "start_time= time.time()\n",
        "dt_classifier1.fit(x_training_features.cpu(),y_train_labels_reshaped)\n",
        "end_time=time.time()\n",
        "print(\"training time=\",end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Make predictions on the testing set\n",
        "from sklearn.metrics import accuracy_score\n",
        "testing_start_time=time.time()\n",
        "y_pred = dt_classifier1.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time=\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test_labels_reshaped,y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "cm = confusion_matrix(y_test_labels_reshaped,y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "# Create the Extra Trees classifier\n",
        "clf = ExtraTreesClassifier(criterion=\"gini\")\n",
        "\n",
        "start_time=time.time()\n",
        "clf.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "#model.fit(, )\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = clf.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "\n",
        "# Create the Extra Trees classifier\n",
        "clf1 = ExtraTreesClassifier(criterion=\"entropy\")\n",
        "\n",
        "start_time=time.time()\n",
        "clf1.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "#model.fit(, )\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = clf1.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped,y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "#print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=100,  # Number of boosting rounds\n",
        "    max_depth=3,  # Maximum depth of each tree\n",
        "    learning_rate=0.001,  # Learning rate (step size shrinkage)\n",
        "    objective='multi:softprob',  # Objective function for binary classification\n",
        "    random_state=42,n_jobs=4,booster =\"gbtree\", gamma = 0\n",
        ")\n",
        "start_time=time.time()\n",
        "model.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = model.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "report = classification_report(y_test_labels_reshaped, y_pred)\n",
        "#print(report)\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity,\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=100,  # Number of boosting rounds\n",
        "    max_depth=3,  # Maximum depth of each tree\n",
        "    learning_rate=0.001,  # Learning rate (step size shrinkage)\n",
        "    objective='multi:softprob',  # Objective function for binary classification\n",
        "    random_state=42,n_jobs=4,booster =\"gbtree\", gamma = 0\n",
        ")\n",
        "start_time=time.time()\n",
        "model.fit(x_training_features.cpu(), y_train_labels_reshaped)\n",
        "stop_time=time.time()\n",
        "print(\"trainingtime=\",stop_time-start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "testing_start_time=time.time()\n",
        "y_pred = model.predict(x_testing_features.cpu())\n",
        "testing_stop_time=time.time()\n",
        "print(\"testing time =\", testing_stop_time-testing_start_time)\n",
        "report = classification_report(y_test_labels_reshaped, y_pred)\n",
        "#print(report)\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test_labels_reshaped, y_pred)\n",
        "\n",
        "print(\"accuracy:\",accuracy)\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision_1 = precision_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "recall=recall_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "fscore=f1_score(y_test_labels_reshaped, y_pred,average='macro')\n",
        "print(\"precision:\",precision_1)\n",
        "print(\"recall\",recall)\n",
        "print(\"f1-Score\",fscore)\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "# Assuming y_true and y_pred are your true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test_labels_reshaped, y_pred)\n",
        "print(cm)\n",
        "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) from the confusion matrix\n",
        "tp = cm[1, 1]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "tn = cm[0, 0]\n",
        "#print(tp,fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "print(\"Specificity:\", specificity,\"sensivity=\",sensitivity)\n",
        "TPR= tp/(tp+fn)\n",
        "FPR=fp/(fp+tn)\n",
        "X=[0,FPR,1]\n",
        "Y=[0,TPR,1]\n",
        "AUCSCORE =np.trapz(Y,X)\n",
        "print(\"AUCSCORE=\", AUCSCORE)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
